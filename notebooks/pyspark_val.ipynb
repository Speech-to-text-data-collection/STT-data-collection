{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3033d49a-ebd2-468e-b8cb-e578b99b235b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 06:52:50.519302: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-14 06:52:50.519331: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe6f0ba-bac6-453d-82c8-dd7746a43ae4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/14 13:56:26 WARN Utils: Your hostname, michael resolves to a loopback address: 127.0.1.1; using 192.168.43.177 instead (on interface wlo1)\n",
      "21/09/14 13:56:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/root/anaconda3/envs/tensorflow/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/14 13:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Collinear Points\")\n",
    "sc = SparkContext('local',conf=conf)    \n",
    "from pyspark.rdd import RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15dfd3b1-1fef-45ae-b858-8ecfeee90d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary rdd file from the audio files\n",
    "binary_wave_rdd = sc.binaryFiles('../data/wav/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78755bb-0738-4b09-837c-863385026743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfomer binary_wave_rdd to a tuple rdd with location of file and numpy array\n",
    "rdd = binary_wave_rdd.map(lambda x : (x[0].split('/')[-1].split('.')[0], librosa.load(io.BytesIO(x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1af4e50-3ac4-4ffb-8f68-238e3fc1616f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CleanAudio():\n",
    "    \"\"\"Clean audio data by removing dead spaces, ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def normalize_audio(self, signal):\n",
    "        feats_mean = np.mean(signal, axis=0)\n",
    "        feats_std = np.std(signal, axis=0)\n",
    "        signal = (signal - feats_mean) / (feats_std + 1e-14)\n",
    "        return signal\n",
    "\n",
    "    def trim_audio(self, signal, trim_db=None):\n",
    "        signal, index = librosa.effects.trim(signal, top_db=trim_db)\n",
    "        return signal\n",
    "\n",
    "    def split_audio(self, signal, clean_db=None):\n",
    "        yt = librosa.effects.split(signal, top_db=clean_db)\n",
    "        clean_signal = []\n",
    "        for start_i, end_i in yt:\n",
    "            clean_signal.append(signal[start_i: end_i])\n",
    "        signal = np.concatenate(np.array(clean_signal), axis=0)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c0ac4c1-149b-4861-8778-8523dd37ef1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "class LogMelgramLayer(Layer):\n",
    "    def __init__(self, num_fft, hop_length, num_mels, sample_rate, f_min, f_max, eps, **kwargs):\n",
    "        super(LogMelgramLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_fft = num_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.num_mels = num_mels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        self.eps = eps\n",
    "        self.num_freqs = num_fft // 2 + 1\n",
    "        lin_to_mel_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=self.num_mels,\n",
    "            num_spectrogram_bins=self.num_freqs,\n",
    "            sample_rate=self.sample_rate,\n",
    "            lower_edge_hertz=self.f_min,\n",
    "            upper_edge_hertz=self.f_max,\n",
    "        )\n",
    "\n",
    "        self.lin_to_mel_matrix = lin_to_mel_matrix\n",
    "\n",
    "    \n",
    "    def call(self, input):\n",
    "\n",
    "        def _tf_log10(x):\n",
    "            numerator = tf.math.log(x)\n",
    "            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "            return numerator / denominator\n",
    "\n",
    "        stfts = tf.signal.stft(\n",
    "            input,\n",
    "            frame_length=self.num_fft,\n",
    "            frame_step=self.hop_length,\n",
    "            pad_end=False,  # librosa test compatibility\n",
    "        )\n",
    "        mag_stfts = tf.abs(stfts)\n",
    "\n",
    "        melgrams = tf.tensordot(  # assuming channel_first, so (b, c, f, t)\n",
    "            tf.square(mag_stfts), self.lin_to_mel_matrix, axes=[2, 0]\n",
    "        )\n",
    "        log_melgrams = _tf_log10(melgrams + self.eps)\n",
    "        return tf.expand_dims(log_melgrams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9327abda-2893-43f7-a356-99a9f9f9388d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "sr = 8000\n",
    "fft_size = 256\n",
    "hop_size = 128\n",
    "n_mels = 128\n",
    "\n",
    "def preprocessing_model(fft_size, hop_size, n_mels):\n",
    "    \n",
    "    input_data = Input(name='input', shape=(None,), dtype=\"float32\")\n",
    "    spec = LogMelgramLayer(\n",
    "        num_fft=fft_size,\n",
    "        hop_length=hop_size,\n",
    "        num_mels=n_mels,\n",
    "        sample_rate=sr,\n",
    "        f_min=0.0,\n",
    "        f_max=sr // 2,\n",
    "        eps=1e-6)(input_data)\n",
    "    x = BatchNormalization(axis=2)(spec)\n",
    "    # x = Permute((2, 1, 3), name='permute', dtype=\"float32\")(x)\n",
    "    model = Model(inputs=input_data, outputs=x, name=\"preprocessin_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_model(melspecModel, output_dim, custom_model, calc=None):\n",
    "\n",
    "    input_audios = Input(name='the_input', shape=(None,))\n",
    "    pre = melspecModel(input_audios)\n",
    "    pre.trainable = False  # Freeze the layer\n",
    "    pre = tf.squeeze(pre, [3])\n",
    "\n",
    "    y_pred = custom_model(pre)\n",
    "    model = Model(inputs=input_audios, outputs=y_pred, name=\"model_builder\")\n",
    "    model.output_length = calc\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def cnn_output_length(input_length, kernel_list, pool_sizes, cnn_stride, mx_stride, padding='same'):\n",
    "\n",
    "    if padding == 'same':\n",
    "        output_length = input_length\n",
    "        for i, j in zip(cnn_stride, pool_sizes):\n",
    "            output_length = (output_length)/i\n",
    "            if j != 0:\n",
    "                output_length = (output_length - j)/mx_stride + 1\n",
    "\n",
    "        return tf.math.ceil(output_length)\n",
    "\n",
    "    elif padding == 'valid':\n",
    "\n",
    "        output_length = input_length\n",
    "        for i, j in zip(kernel_list, pool_sizes):\n",
    "            output_length = (output_length - i)/cnn_stride + 1\n",
    "            if j != 0:\n",
    "                output_length = (output_length - j)/mx_stride + 1\n",
    "\n",
    "        return tf.math.floor(output_length)\n",
    "\n",
    "\n",
    "def block(filters, inp):\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = LeakyReLU(.1)(x)\n",
    "    x = Dropout(.4)(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(.1)(x)\n",
    "    x = Dropout(.4)(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    return(x)\n",
    "\n",
    "\n",
    "def resnet(input_dim, output_dim=224, units=256,  num_birnn=2):\n",
    "\n",
    "    filters = [32, 32, 32]\n",
    "    kernels = [3, 3, 3]\n",
    "    pool_sizes = [0, 0, 2]\n",
    "    cnn_stride = [1, 1, 1]\n",
    "    mx_stride = 2\n",
    "\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    x = Reshape((-1, input_dim, 1), dtype=\"float32\")(input_data)\n",
    "\n",
    "    x = Conv2D(filters[0], (3, 3), padding='same')(x)\n",
    "    x = MaxPooling2D((1, 2), strides=(1, 2), padding='same')(x)\n",
    "\n",
    "    x = Add()([block(filters[0], x), x])\n",
    "    x = Add()([block(filters[0], x), x])\n",
    "    x = Add()([block(filters[0], x), x])\n",
    "\n",
    "    x = Conv2D(filters[1], (3, 3), padding='same')(x)\n",
    "    x = MaxPooling2D((1, 2), strides=(1, 2), padding='same')(x)\n",
    "\n",
    "    x = Add()([block(filters[1], x), x])\n",
    "    x = Add()([block(filters[1], x), x])\n",
    "    x = Add()([block(filters[1], x), x])\n",
    "\n",
    "    x = Conv2D(filters[2], (3, 3), padding='same')(x)\n",
    "    x = MaxPooling2D((1, 2), strides=(1, 2), padding='same')(x)\n",
    "\n",
    "    x = Add()([block(filters[2], x), x])\n",
    "    x = Add()([block(filters[2], x), x])\n",
    "    x = Add()([block(filters[2], x), x])\n",
    "\n",
    "    # x = MaxPooling2D((2,2), strides=2, padding = 'same')(x)\n",
    "    x = AveragePooling2D((2, 2), strides=2, padding='same')(x)\n",
    "    x = Reshape((-1, x.shape[-1] * x.shape[-2]))(x)\n",
    "\n",
    "    # GRULayer\n",
    "    for i in range(num_birnn):\n",
    "        x = Bidirectional(GRU(units=units, return_sequences=True,\n",
    "                          implementation=2, name='rnn_{}'.format(i)))(x)\n",
    "        x = Dropout(.4)(x)\n",
    "        x = LeakyReLU(.1)(x)\n",
    "        x = BatchNormalization(name='bn_rnn_{}'.format(i))(x)\n",
    "\n",
    "    x = TimeDistributed(Dense(output_dim))(x)\n",
    "    y_pred = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=y_pred, name=\"custom_model\")\n",
    "\n",
    "    def output_length_calculater(x): return cnn_output_length(\n",
    "        x, kernels, pool_sizes, cnn_stride, mx_stride)\n",
    "\n",
    "    return model, output_length_calculater\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "945f7d81-93da-42f7-8660-e0a75dc851bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "class TokenizerWrap(Tokenizer):\n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        words = [self.index_to_word[token] for token in tokens if token != 0]\n",
    "        text = \"\".join(words)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971b1648-a125-4847-8d40-938f5118b9c7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "handle = open('../models/char_tokenizer_amharic.pickle', 'rb')\n",
    "tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f0c2c7e-4d31-4068-82ee-f4b930c6d467",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import librosa\n",
    "import warnings\n",
    "from jiwer import wer\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.utils import shuffle\n",
    "from copy import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class Predict():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.clean_audio = CleanAudio()\n",
    "\n",
    "    def get_audio(self, audio_file):\n",
    "        sr = 8000\n",
    "        wav, rate = audio_file\n",
    "        y = librosa.resample(wav, rate, sr)\n",
    "        return y\n",
    "\n",
    "    def get_clean_audio(self, wav):\n",
    "        y = self.clean_audio.normalize_audio(wav)\n",
    "        y = self.clean_audio.split_audio(y, 30)\n",
    "        return y\n",
    "\n",
    "    def predict(self, audio_signal):\n",
    "        y = audio_signal.reshape(1, -1)\n",
    "        fft_size = 256\n",
    "        hop_size = 128\n",
    "        n_mels = 128\n",
    "        melspecModel = preprocessing_model(fft_size, hop_size, n_mels)\n",
    "        resnet_, calc = resnet(n_mels, 224, 512, 4)\n",
    "        model = build_model(melspecModel, 224, resnet_, calc)\n",
    "        model.load_weights('../models/resnet_v3.h5')\n",
    "#         models =tf.keras.models.clone_model(model)\n",
    "#         models.load_weights('../models/resnet_v3.h5')\n",
    "        y_pred = model.predict(y)\n",
    "\n",
    "        input_shape = tf.keras.backend.shape(y_pred)\n",
    "        input_length = tf.ones(\n",
    "            shape=input_shape[0]) * tf.keras.backend.cast(input_shape[1], 'float32')\n",
    "        prediction = tf.keras.backend.ctc_decode(\n",
    "            y_pred, input_length, greedy=False)[0][0]\n",
    "\n",
    "        pred = K.eval(prediction).flatten().tolist()\n",
    "        pred = list(filter(lambda a: a != -1, pred))\n",
    "#         handle = open('../models/char_tokenizer_amharic.pickle', 'rb', encoding='UTF-8')\n",
    "#         tokenizer = pickle.load(handle)\n",
    "        return ''.join(tokenizer.tokens_to_string(pred))\n",
    "\n",
    "def validate(rdd):\n",
    "    \n",
    "    predict = Predict()\n",
    "    audio_file_rdd = rdd.map(lambda x : (x[0], predict.get_audio(x[1])))\n",
    "#     coll_aud = audio_file_rdd.collect()\n",
    "#     print(coll_aud[0])\n",
    "    clean_audio_file_rdd = audio_file_rdd.map(lambda x : (x[0], predict.get_clean_audio(x[1])))\n",
    "#     coll_file = clean_audio_file_rdd.collect()\n",
    "#     print(coll_file[0])\n",
    "    predicted_txt_rdd = clean_audio_file_rdd.map(lambda x: (x[0], predict.predict(x[1])))\n",
    "#     coll_pred = predicted_txt_rdd.collect()\n",
    "    return predicted_txt_rdd, clean_audio_file_rdd\n",
    "\n",
    "predicted_rdd, clean_audio_file_rdd = validate(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1810bad1-be73-4950-bf75-1181ce85cfcc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 10:07:21.776073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-14 10:07:21.776115: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/tmp/ipykernel_2706/2505260044.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# get collection of audio wave file and turn it to dictionary\n",
    "coll_clean = clean_audio_file_rdd.collect()\n",
    "dct_clean = dict((y, x) for y, x in coll_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbec4a0a-1962-438b-b698-d4bdd0464c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite clean audio to file\n",
    "from scipy.io.wavfile import write\n",
    "import scipy.io.wavfile\n",
    "for i,j in dct_clean.items():\n",
    "    scipy.io.wavfile.write('../data/test_data/audio_test/'+i+'.wav', 8000,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32dd1431-2bdd-4e3b-8380-e81e8f3dc843",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 10:07:55.831901: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-09-14 10:07:55.831933: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-09-14 10:07:55.831956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-53): /proc/driver/nvidia/version does not exist\n",
      "2021-09-14 10:07:55.832212: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method LogMelgramLayer.call of <__main__.LogMelgramLayer object at 0x7f4264052fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to locate the source code of <bound method LogMelgramLayer.call of <__main__.LogMelgramLayer object at 0x7f4264052fa0>>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2021-09-14 10:08:00.633174: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "/tmp/ipykernel_2706/2505260044.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4248349d30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4249ac8dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get collection of predicted amharic txt with it's audio name\n",
    "coll_pred = predicted_rdd. collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce1a9646-ce91-41e9-8154-44f6c47502ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# turn collection of prediction to collection of dictionary\n",
    "dct_pred = dict((y, x) for y, x in coll_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "337fd9af-bbd0-4bb7-8588-8c97feb966bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_tr_10000_tr097082': 'የተላይ የትግራይ አወራጃ ተወላጆች ገንዘባቸውን አዋድተው የልማት ተቋማትን እንዲመሰሩቱ ትልማ አይፈግድ ም',\n",
       " '2_tr_10001_tr097083': 'የጠመንጃ ተኩስ ተከፈተና አራት የኤርትራ ወታደሮች ተገደሉ',\n",
       " '3_tr_10002_tr097084': 'ላነሷቸው ጥያቄዎች የሰጡትን መልስ አቅርበ ነዋል',\n",
       " '4_tr_10003_tr097085': 'እብዱ አስፋልቱ ላይየ ኰለኰ ለው ድንጋይ መኪና አላሳልፍ አለ',\n",
       " '5_tr_10004_tr097086': 'ጠጁን ኰ መኰ መ ኰ መ ኰ መና ሚስቱን ሲ ያሰቃያት አደረ',\n",
       " '6_tr_10005_tr097087': 'ድንቹ በድንብ ስለተኰተቆተ በጥሩ ሁኔታ ኰረተ',\n",
       " '7_tr_10006_tr097088': 'በድህነቱ ላይ ይህ ክፉ በሽታ ስለያዘው ሰውነቱ በጣም ኰ ሰ',\n",
       " '8_tr_10007_tr097089': 'በሩን እንዲህ በሀይል አታንኳኲ ብዬ አልነበረም እንዴ'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36bf145e-7fdf-4820-bba7-35ffd38b9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original text file .json\n",
    "import json\n",
    "f = open('../data/test_data/data.json')\n",
    "text = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b658aa-bd90-4733-b9db-57afd25ae9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6fe71736-b995-4087-8486-46153e68a243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "0.4444444444444444\n",
      "0.375\n",
      "0.6153846153846154\n",
      "0.5333333333333333\n",
      "0.625\n",
      "0.4166666666666667\n",
      "0.6\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Get invalid audios\n",
    "bad_aud = []\n",
    "from jiwer import wer\n",
    "for j,k in dct_pred.items():\n",
    "    for l,m in text.items():\n",
    "        ids = j.split('_')[0]\n",
    "        if l == ids:\n",
    "            error = wer(m,k)\n",
    "            print(error)\n",
    "            if error < 0.6:\n",
    "                bad_aud.append(j)\n",
    "print(bad_aud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4e81ec7-f131-4b3b-be7a-1c1b224dd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove audios that invalid\n",
    "import os\n",
    "for i in bad_aud:\n",
    "    try:\n",
    "        os.remove('../data/test_data/audio_test/'+i+'.wav')\n",
    "    except FileNotFoundError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d74aa-0739-449a-aafd-796f257e406d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38831e-dd33-4949-b10f-05cc6114f8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
